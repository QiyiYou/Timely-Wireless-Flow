ss=1,ss_vec=[ 1, 1, 1, ], ss_back=1
ss=2,ss_vec=[ 1, 1, 2, ], ss_back=2
ss=3,ss_vec=[ 1, 2, 1, ], ss_back=3
ss=4,ss_vec=[ 1, 2, 2, ], ss_back=4
ss=5,ss_vec=[ 2, 1, 1, ], ss_back=5
ss=6,ss_vec=[ 2, 1, 2, ], ss_back=6
ss=7,ss_vec=[ 2, 2, 1, ], ss_back=7
ss=8,ss_vec=[ 2, 2, 2, ], ss_back=8

Flow 1: (offset, period, delay, success_prob) = (0, 3, 2, 0.820000), arrival_prob = (1.000000, )

Flow 2: (offset, period, delay, success_prob) = (0, 3, 3, 0.970000), arrival_prob = (1.000000, )

Flow 3: (offset, period, delay, success_prob) = (0, 3, 3, 0.690000), arrival_prob = (1.000000, )

weighted_sum, utility_coeff=[1.000000,17.000000,19.000000,]
tt=3
optimal_policy(3,1)=1, optimal_reward(3,1)=0.000000
optimal_policy(3,2)=3, optimal_reward(3,2)=13.110000
optimal_policy(3,3)=2, optimal_reward(3,3)=16.490000
optimal_policy(3,4)=2, optimal_reward(3,4)=16.490000
optimal_policy(3,5)=1, optimal_reward(3,5)=0.820000
optimal_policy(3,6)=3, optimal_reward(3,6)=13.110000
optimal_policy(3,7)=2, optimal_reward(3,7)=16.490000
optimal_policy(3,8)=2, optimal_reward(3,8)=16.490000
tt=2
optimal_policy(2,1)=1, optimal_reward(2,1)=0.000000
optimal_policy(2,2)=3, optimal_reward(2,2)=17.174100
optimal_policy(2,3)=2, optimal_reward(2,3)=16.984700
optimal_policy(2,4)=2, optimal_reward(2,4)=29.701400
optimal_policy(2,5)=1, optimal_reward(2,5)=0.820000
optimal_policy(2,6)=3, optimal_reward(2,6)=17.174100
optimal_policy(2,7)=1, optimal_reward(2,7)=17.310000
optimal_policy(2,8)=2, optimal_reward(2,8)=29.701400
tt=1
optimal_policy(1,1)=1, optimal_reward(1,1)=0.000000
optimal_policy(1,2)=3, optimal_reward(1,2)=18.433971
optimal_policy(1,3)=2, optimal_reward(1,3)=16.999541
optimal_policy(1,4)=2, optimal_reward(1,4)=34.039919
optimal_policy(1,5)=1, optimal_reward(1,5)=0.967600
optimal_policy(1,6)=3, optimal_reward(1,6)=18.999771
optimal_policy(1,7)=1, optimal_reward(1,7)=17.863254
optimal_policy(1,8)=3, optimal_reward(1,8)=34.261334
finish fullBackwardInduction with elapsed time 0.522021
