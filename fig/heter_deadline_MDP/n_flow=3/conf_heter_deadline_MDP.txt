ss=1,ss_vec=[ 1, 1, 1, ], ss_back=1
ss=2,ss_vec=[ 1, 1, 2, ], ss_back=2
ss=3,ss_vec=[ 1, 2, 1, ], ss_back=3
ss=4,ss_vec=[ 1, 2, 2, ], ss_back=4
ss=5,ss_vec=[ 2, 1, 1, ], ss_back=5
ss=6,ss_vec=[ 2, 1, 2, ], ss_back=6
ss=7,ss_vec=[ 2, 2, 1, ], ss_back=7
ss=8,ss_vec=[ 2, 2, 2, ], ss_back=8

Flow 1: (offset, period, delay, success_prob) = (0, 3, 2, 0.080000), arrival_prob = (1.000000, )

Flow 2: (offset, period, delay, success_prob) = (0, 3, 3, 1.000000), arrival_prob = (1.000000, )

Flow 3: (offset, period, delay, success_prob) = (0, 3, 3, 0.690000), arrival_prob = (1.000000, )

weighted_sum, utility_coeff=[1.000000,1.000000,1.000000,]
tt=3
optimal_policy(3,1)=1, optimal_reward(3,1)=0.000000
optimal_policy(3,2)=3, optimal_reward(3,2)=0.690000
optimal_policy(3,3)=2, optimal_reward(3,3)=1.000000
optimal_policy(3,4)=2, optimal_reward(3,4)=1.000000
optimal_policy(3,5)=1, optimal_reward(3,5)=0.080000
optimal_policy(3,6)=3, optimal_reward(3,6)=0.690000
optimal_policy(3,7)=2, optimal_reward(3,7)=1.000000
optimal_policy(3,8)=2, optimal_reward(3,8)=1.000000
tt=2
optimal_policy(2,1)=1, optimal_reward(2,1)=0.000000
optimal_policy(2,2)=3, optimal_reward(2,2)=0.903900
optimal_policy(2,3)=1, optimal_reward(2,3)=1.000000
optimal_policy(2,4)=2, optimal_reward(2,4)=1.690000
optimal_policy(2,5)=1, optimal_reward(2,5)=0.080000
optimal_policy(2,6)=3, optimal_reward(2,6)=0.903900
optimal_policy(2,7)=1, optimal_reward(2,7)=1.080000
optimal_policy(2,8)=2, optimal_reward(2,8)=1.690000
tt=1
optimal_policy(1,1)=1, optimal_reward(1,1)=0.000000
optimal_policy(1,2)=3, optimal_reward(1,2)=0.970209
optimal_policy(1,3)=1, optimal_reward(1,3)=1.000000
optimal_policy(1,4)=2, optimal_reward(1,4)=1.903900
optimal_policy(1,5)=1, optimal_reward(1,5)=0.153600
optimal_policy(1,6)=3, optimal_reward(1,6)=1.025409
optimal_policy(1,7)=1, optimal_reward(1,7)=1.153600
optimal_policy(1,8)=3, optimal_reward(1,8)=1.959100
finish fullBackwardInduction with elapsed time 0.529032
